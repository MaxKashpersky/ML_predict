"""
–ú–æ–¥—É–ª—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ —Ä–∞—Å—á–µ—Ç–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
"""

import pandas as pd
import numpy as np
import ta
import logging
from typing import Tuple, List, Optional, Dict
from config import config


class DataPreprocessor:
    def __init__(self, verbose: bool = True):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞"""
        self.verbose = verbose
        self.setup_logging()
        self.indicators_cache = {}

    def setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        self.logger = logging.getLogger(__name__)
        if self.verbose:
            self.logger.setLevel(logging.INFO)
        else:
            self.logger.setLevel(logging.WARNING)

    def log(self, message: str, level: str = 'info'):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π"""
        if self.verbose:
            if level == 'info':
                self.logger.info(message)
            elif level == 'error':
                self.logger.error(message)
            elif level == 'warning':
                self.logger.warning(message)
            elif level == 'debug':
                self.logger.debug(message)

    def calculate_all_indicators(self, df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:
        """
        –†–∞—Å—á–µ—Ç –≤—Å–µ—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤

        Args:
            df: DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ OHLCV
            verbose: –§–ª–∞–≥ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

        Returns:
            DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏
        """
        try:
            if df.empty:
                self.log("Empty DataFrame for indicator calculation", 'warning')
                return df

            data = df.copy()

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            required_columns = ['open', 'high', 'low', 'close', 'volume']
            missing_columns = [col for col in required_columns if col not in data.columns]
            if missing_columns:
                self.log(f"Missing columns: {missing_columns}", 'error')
                return data

            self.log(f"Calculating indicators for {len(data)} candles", 'debug')

            # 1. –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
            data['SMA_20'] = ta.trend.sma_indicator(data['close'], window=20)
            data['SMA_50'] = ta.trend.sma_indicator(data['close'], window=50)
            data['SMA_100'] = ta.trend.sma_indicator(data['close'], window=100)
            data['SMA_200'] = ta.trend.sma_indicator(data['close'], window=200)

            data['EMA_12'] = ta.trend.ema_indicator(data['close'], window=12)
            data['EMA_26'] = ta.trend.ema_indicator(data['close'], window=26)
            data['EMA_50'] = ta.trend.ema_indicator(data['close'], window=50)

            # 2. –ò–Ω–¥–µ–∫—Å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π —Å–∏–ª—ã (RSI)
            data['RSI_14'] = ta.momentum.rsi(data['close'], window=14)
            data['RSI_7'] = ta.momentum.rsi(data['close'], window=7)

            # 3. Stochastic Oscillator
            stoch = ta.momentum.StochasticOscillator(data['high'], data['low'], data['close'],
                                                     window=14, smooth_window=3)
            data['STOCH_K'] = stoch.stoch()
            data['STOCH_D'] = stoch.stoch_signal()

            # 4. MACD
            macd = ta.trend.MACD(data['close'], window_slow=26, window_fast=12, window_sign=9)
            data['MACD'] = macd.macd()
            data['MACD_SIGNAL'] = macd.macd_signal()
            data['MACD_DIFF'] = macd.macd_diff()

            # 5. Bollinger Bands
            bb = ta.volatility.BollingerBands(data['close'], window=20, window_dev=2)
            data['BB_UPPER'] = bb.bollinger_hband()
            data['BB_MIDDLE'] = bb.bollinger_mavg()
            data['BB_LOWER'] = bb.bollinger_lband()
            data['BB_WIDTH'] = (data['BB_UPPER'] - data['BB_LOWER']) / data['BB_MIDDLE']
            data['BB_PCT'] = (data['close'] - data['BB_LOWER']) / (data['BB_UPPER'] - data['BB_LOWER'])

            # 6. Average True Range (ATR)
            data['ATR_14'] = ta.volatility.average_true_range(data['high'], data['low'], data['close'], window=14)

            # 7. Parabolic SAR
            psar = ta.trend.PSARIndicator(data['high'], data['low'], data['close'])
            data['PSAR'] = psar.psar()

            # 8. Commodity Channel Index (CCI)
            data['CCI_20'] = ta.trend.cci(data['high'], data['low'], data['close'], window=20)

            # 9. Awesome Oscillator
            data['AO'] = ta.momentum.awesome_oscillator(data['high'], data['low'])

            # 10. Williams %R
            data['WILLIAMS_R'] = ta.momentum.williams_r(data['high'], data['low'], data['close'], lbp=14)

            # 11. Rate of Change (ROC)
            data['ROC_10'] = ta.momentum.roc(data['close'], window=10)
            data['ROC_20'] = ta.momentum.roc(data['close'], window=20)

            # 12. Money Flow Index (MFI)
            data['MFI_14'] = ta.volume.money_flow_index(data['high'], data['low'], data['close'], data['volume'],
                                                        window=14)

            # 13. On-Balance Volume (OBV)
            data['OBV'] = ta.volume.on_balance_volume(data['close'], data['volume'])

            # 14. Volume Weighted Average Price (VWAP)
            # –î–ª—è VWAP –Ω—É–∂–Ω—ã –¥–∞–Ω–Ω—ã–µ –∑–∞ –¥–µ–Ω—å, –ø–æ—ç—Ç–æ–º—É —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ –µ—Å–ª–∏ –µ—Å—Ç—å –¥–Ω–µ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

            # 15. Ichimoku Cloud (—á–∞—Å—Ç–∏—á–Ω–æ)
            ichimoku = ta.trend.IchimokuIndicator(data['high'], data['low'])
            data['ICHIMOKU_CONVERSION'] = ichimoku.ichimoku_conversion_line()
            data['ICHIMOKU_BASE'] = ichimoku.ichimoku_base_line()
            data['ICHIMOKU_A'] = ichimoku.ichimoku_a()
            data['ICHIMOKU_B'] = ichimoku.ichimoku_b()

            # 16. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∏—á–∏
            data['RETURNS'] = data['close'].pct_change()
            data['LOG_RETURNS'] = np.log(data['close'] / data['close'].shift(1))
            data['VOLATILITY_20'] = data['RETURNS'].rolling(window=20).std()
            data['VOLUME_MA_20'] = data['volume'].rolling(window=20).mean()
            data['PRICE_RANGE'] = (data['high'] - data['low']) / data['close']
            data['BODY_SIZE'] = (data['close'] - data['open']) / data['close']

            # 17. –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏
            data['HOUR'] = data.index.hour
            data['DAY_OF_WEEK'] = data.index.dayofweek
            data['MONTH'] = data.index.month

            # 18. –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            # –°–æ–∑–¥–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å —Ä–∞–∑–Ω—ã–º–∏ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–º–∏

            # –ü–ï–†–í–û–ï: –ü—Ä–æ–≤–µ—Ä–∏–º –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
            volatility = data['close'].pct_change().std() * 100  # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
            avg_return = abs(data['close'].pct_change().mean()) * 100

            self.log(f"Data statistics:", 'info')
            self.log(f"  Volatility (std of returns): {volatility:.2f}%", 'info')
            self.log(f"  Average absolute return: {avg_return:.4f}%", 'info')

            # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π
            returns = data['close'].pct_change().dropna()
            percentiles = np.percentile(returns * 100, [10, 25, 50, 75, 90])
            self.log(f"  Return percentiles (10%, 25%, 50%, 75%, 90%): {percentiles}", 'info')

            # –†–ï–ê–õ–ò–°–¢–ò–ß–ù–´–ï –ü–û–†–û–ì–ò –ù–ê –û–°–ù–û–í–ï –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–Ø –î–ê–ù–ù–´–•
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –ø–æ—Ä–æ–≥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª–µ–π
            # –¶–µ–ª—å: –ø—Ä–∏–º–µ—Ä–Ω–æ 30% LONG, 40% HOLD, 30% SHORT

            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: 1 - —Ä–æ—Å—Ç, 0 - –±–æ–∫–æ–≤–∏–∫, -1 - –ø–∞–¥–µ–Ω–∏–µ
            for horizon in [1, 3, 5, 10]:  # 1, 3, 5, 10 —Å–≤–µ—á–µ–π –≤–ø–µ—Ä–µ–¥
                future_return = data['close'].pct_change(horizon).shift(-horizon)
                target_name = f'TARGET_CLASS_{horizon}'

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
                if future_return.dropna().empty:
                    self.log(f"Skipping {target_name}: insufficient data", 'warning')
                    continue

                # –°–¢–†–ê–¢–ï–ì–ò–Ø 1: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª–∏ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
                try:
                    # –¶–µ–ª–µ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: 30% LONG, 40% HOLD, 30% SHORT
                    threshold_up = np.percentile(future_return.dropna(), 70)  # –í–µ—Ä—Ö–Ω–∏–µ 30%
                    threshold_down = np.percentile(future_return.dropna(), 30)  # –ù–∏–∂–Ω–∏–µ 30%

                    # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –ø–æ—Ä–æ–≥–∏
                    min_threshold = max(volatility * 0.5 / 100, 0.001)  # –ü–æ–ª–æ–≤–∏–Ω–∞ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏, –º–∏–Ω–∏–º—É–º 0.1%
                    threshold_up = max(threshold_up, min_threshold)
                    threshold_down = min(threshold_down, -min_threshold)

                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ (–Ω–µ –±–æ–ª–µ–µ 2% –¥–ª—è —Å–∫–∞–ª—å–ø–∏–Ω–≥–∞)
                    max_threshold = 0.02  # 2% –º–∞–∫—Å–∏–º—É–º
                    threshold_up = min(threshold_up, max_threshold)
                    threshold_down = max(threshold_down, -max_threshold)

                    # –õ–æ–≥–∏—Ä—É–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –ø–æ—Ä–æ–≥–∏
                    if verbose:
                        self.log(f"Creating target {target_name}:", 'info')
                        self.log(
                            f"  Using percentiles: LONG > {threshold_up * 100:.3f}%, SHORT < {threshold_down * 100:.3f}%",
                            'info')
                        self.log(
                            f"  (70th percentile: {threshold_up * 100:.3f}%, 30th percentile: {threshold_down * 100:.3f}%)",
                            'info')

                    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏
                    data[target_name] = 0  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é HOLD
                    data.loc[future_return > threshold_up, target_name] = 1  # LONG
                    data.loc[future_return < threshold_down, target_name] = -1  # SHORT

                except Exception as percentile_error:
                    self.log(f"Percentile method failed for {target_name}: {percentile_error}", 'warning')
                    # –°–¢–†–ê–¢–ï–ì–ò–Ø 2: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
                    base_threshold = volatility * 0.8 / 100  # 80% –æ—Ç –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
                    threshold_up = max(base_threshold, 0.002)  # –ú–∏–Ω–∏–º—É–º 0.2%
                    threshold_down = -threshold_up

                    if verbose:
                        self.log(f"  Fallback to volatility-based thresholds: ¬±{threshold_up * 100:.3f}%", 'info')

                    data[target_name] = 0
                    data.loc[future_return > threshold_up, target_name] = 1
                    data.loc[future_return < threshold_down, target_name] = -1

                # –õ–æ–≥–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
                class_counts = data[target_name].value_counts()
                total = len(data[target_name].dropna())

                if verbose and total > 0:
                    self.log(f"Target distribution {target_name}:", 'info')
                    for cls in [-1, 0, 1]:
                        count = class_counts.get(cls, 0)
                        percentage = (count / total) * 100
                        self.log(f"  Class {cls}: {count} ({percentage:.1f}%)", 'info')

                    # –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï –µ—Å–ª–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ
                    hold_percentage = class_counts.get(0, 0) / total * 100
                    if hold_percentage > 90:
                        self.log(f"‚ö†Ô∏è  –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ HOLD –º–µ—Ç–æ–∫ ({hold_percentage:.1f}%)!",
                                 'error')
                        self.log(f"‚ö†Ô∏è  –î–∞–Ω–Ω—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å —Å–ª–∏—à–∫–æ–º —Å—Ç–∞–±–∏–ª—å–Ω—ã–º–∏ –∏–ª–∏ –ø–µ—Ä–∏–æ–¥ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π", 'error')
                        self.log(f"‚ö†Ô∏è  –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:", 'error')
                        self.log(f"‚ö†Ô∏è    1. –£–≤–µ–ª–∏—á—å—Ç–µ –ø–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö (–º–∏–Ω–∏–º—É–º 6 –º–µ—Å—è—Ü–µ–≤)", 'error')
                        self.log(f"‚ö†Ô∏è    2. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–æ–ª—å—à–∏–π —Ç–∞–π–º—Ñ—Ä–µ–π–º (1h –≤–º–µ—Å—Ç–æ 5m)", 'error')
                        self.log(f"‚ö†Ô∏è    3. –í—ã–±–µ—Ä–∏—Ç–µ –±–æ–ª–µ–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω—É—é –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—É", 'error')

                        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: —Å–∫–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π?
                        large_moves = len(future_return[abs(future_return) > threshold_up])
                        self.log(f"‚ö†Ô∏è  –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –≤—Å–µ–≥–æ {large_moves} –¥–≤–∏–∂–µ–Ω–∏–π > {threshold_up * 100:.2f}%", 'error')

                    elif hold_percentage > 80:
                        self.log(f"‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –º–Ω–æ–≥–æ HOLD –º–µ—Ç–æ–∫ ({hold_percentage:.1f}%)", 'warning')
                        self.log(f"‚ö†Ô∏è  –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø–µ—Ä–∏–æ–¥–∞ –¥–∞–Ω–Ω—ã—Ö", 'warning')

                    elif hold_percentage < 40:
                        self.log(f"‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –º–∞–ª–æ HOLD –º–µ—Ç–æ–∫ ({hold_percentage:.1f}%)", 'warning')
                        self.log(f"‚ö†Ô∏è  –ü–æ—Ä–æ–≥–∏ –º–æ–≥—É—Ç –±—ã—Ç—å —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–º–∏", 'warning')

            # 19. –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            # (–µ—Å–ª–∏ —Ü–µ–Ω–æ–≤—ã–µ –º–µ—Ç–∫–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç)

            # –°–æ–∑–¥–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            if verbose:
                self.log(f"Creating technical indicator targets...", 'info')

            # RSI —Å–∏–≥–Ω–∞–ª—ã
            data['RSI_SIGNAL'] = 0
            data.loc[data['RSI_14'] < 30, 'RSI_SIGNAL'] = 1  # Oversold -> –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π LONG
            data.loc[data['RSI_14'] > 70, 'RSI_SIGNAL'] = -1  # Overbought -> –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π SHORT

            # MACD —Å–∏–≥–Ω–∞–ª—ã (–ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ)
            data['MACD_CROSS'] = 0
            # –ë—ã—Å—Ç—Ä–æ–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–≤–µ—Ä—Ö—É –≤–Ω–∏–∑
            macd_cross_up = (data['MACD'] > data['MACD_SIGNAL']) & (
                        data['MACD'].shift(1) <= data['MACD_SIGNAL'].shift(1))
            macd_cross_down = (data['MACD'] < data['MACD_SIGNAL']) & (
                        data['MACD'].shift(1) >= data['MACD_SIGNAL'].shift(1))
            data.loc[macd_cross_up, 'MACD_CROSS'] = 1
            data.loc[macd_cross_down, 'MACD_CROSS'] = -1

            # Bollinger Bands —Å–∏–≥–Ω–∞–ª—ã
            data['BB_SIGNAL'] = 0
            data.loc[data['close'] < data['BB_LOWER'] * 0.99, 'BB_SIGNAL'] = 1  # –ù–∏–∂–µ –Ω–∏–∂–Ω–µ–π –ø–æ–ª–æ—Å—ã -> LONG
            data.loc[data['close'] > data['BB_UPPER'] * 1.01, 'BB_SIGNAL'] = -1  # –í—ã—à–µ –≤–µ—Ä—Ö–Ω–µ–π –ø–æ–ª–æ—Å—ã -> SHORT

            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Å–∏–≥–Ω–∞–ª
            data['TECH_TARGET'] = 0
            # –î–ª—è LONG: –º–∏–Ω–∏–º—É–º 2 –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ –≥–æ–≤–æ—Ä—è—Ç LONG
            long_signals = (data['RSI_SIGNAL'] == 1).astype(int) + \
                           (data['MACD_CROSS'] == 1).astype(int) + \
                           (data['BB_SIGNAL'] == 1).astype(int)

            # –î–ª—è SHORT: –º–∏–Ω–∏–º—É–º 2 –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ –≥–æ–≤–æ—Ä—è—Ç SHORT
            short_signals = (data['RSI_SIGNAL'] == -1).astype(int) + \
                            (data['MACD_CROSS'] == -1).astype(int) + \
                            (data['BB_SIGNAL'] == -1).astype(int)

            data.loc[long_signals >= 2, 'TECH_TARGET'] = 1
            data.loc[short_signals >= 2, 'TECH_TARGET'] = -1

            if verbose:
                tech_counts = data['TECH_TARGET'].value_counts()
                tech_total = len(data['TECH_TARGET'].dropna())
                self.log(f"Technical target distribution (TECH_TARGET):", 'info')
                for cls in [-1, 0, 1]:
                    count = tech_counts.get(cls, 0)
                    percentage = (count / tech_total) * 100 if tech_total > 0 else 0
                    self.log(f"  Class {cls}: {count} ({percentage:.1f}%)", 'info')

            # 20. –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
            for horizon in [1, 3, 5, 10]:
                future_return = data['close'].pct_change(horizon).shift(-horizon)
                data[f'TARGET_REG_{horizon}'] = future_return

            # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
            initial_len = len(data)
            data = data.dropna()
            removed_count = initial_len - len(data)

            if verbose:
                self.log(f"Indicators calculated: added {len(data.columns) - len(df.columns)} columns")
                self.log(f"Removed {removed_count} rows with NaN values")
                self.log(f"Remaining {len(data)} rows")

                # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç –æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –º–µ—Ç–æ–∫
                self.log(f"Final target distributions:", 'info')
                for horizon in [1, 3, 5, 10]:
                    target_col = f'TARGET_CLASS_{horizon}'
                    if target_col in data.columns:
                        counts = data[target_col].value_counts()
                        total = len(data[target_col])
                        self.log(f"  {target_col}:", 'info')
                        for cls in sorted(counts.index):
                            percentage = (counts[cls] / total) * 100
                            signal = {1: 'LONG', 0: 'HOLD', -1: 'SHORT'}.get(cls, 'UNKNOWN')
                            self.log(f"    {signal}: {counts[cls]} ({percentage:.1f}%)", 'info')

            temporal_features = ['HOUR', 'DAY_OF_WEEK', 'MONTH']
            for temp_feature in temporal_features:
                if temp_feature in data.columns:
                    data = data.drop(columns=[temp_feature])
                    if verbose:
                        self.log(f"Removed temporal feature: {temp_feature}", 'debug')

            # –ö–µ—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            cache_key = f"{len(df)}_{df.index[-1]}"
            self.indicators_cache[cache_key] = data.copy()

            return data

        except Exception as e:
            self.log(f"Error calculating indicators: {e}", 'error')
            import traceback
            self.log(f"Traceback: {traceback.format_exc()}", 'error')
            return df

    def prepare_features_for_training(self, df: pd.DataFrame,
                                      target_column: str = 'TARGET_CLASS_5',
                                      lookback_window: int = 60,
                                      verbose: bool = True) -> Tuple[np.ndarray, np.ndarray]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏—á–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

        Args:
            df: DataFrame —Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏
            target_column: –ù–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏
            lookback_window: –û–∫–Ω–æ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
            verbose: –§–ª–∞–≥ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

        Returns:
            X, y –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        """
        try:
            if target_column not in df.columns:
                self.log(f"Target column {target_column} not found", 'error')
                return np.array([]), np.array([])

            # –õ–æ–≥–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
            class_counts = df[target_column].value_counts()
            total = len(df[target_column])
            self.log(f"Target distribution for {target_column}:", 'info')
            for cls in [-1, 0, 1]:
                count = class_counts.get(cls, 0)
                percentage = (count / total) * 100
                self.log(f"  Class {cls}: {count} ({percentage:.1f}%)", 'info')

                # –í–ê–ñ–ù–û–ï –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï
                if cls == 0 and percentage > 80:
                    self.log(f"‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ HOLD –º–µ—Ç–æ–∫ ({percentage:.1f}%)!", 'warning')
                    self.log(f"‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –±—É–¥–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –≤ –æ—Å–Ω–æ–≤–Ω–æ–º HOLD!", 'warning')
                    self.log(f"‚ö†Ô∏è  –£–≤–µ–ª–∏—á—å—Ç–µ –ø–æ—Ä–æ–≥–∏ –≤ calculate_all_indicators!", 'warning')

            # –û—Ç–¥–µ–ª—è–µ–º —Ñ–∏—á–∏ –∏ —Ç–∞—Ä–≥–µ—Ç—ã
            feature_columns = [col for col in df.columns
                               if not col.startswith('TARGET_')
                               and col not in ['open', 'high', 'low', 'close', 'volume']]

            X_data = df[feature_columns].values
            y_data = df[target_column].values

            # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è LSTM
            X_sequences = []
            y_sequences = []

            for i in range(lookback_window, len(X_data)):
                X_sequences.append(X_data[i - lookback_window:i])
                y_sequences.append(y_data[i])

            X = np.array(X_sequences)
            y = np.array(y_sequences)

            if verbose:
                self.log(f"Created {len(X)} sequences")
                self.log(f"X shape: {X.shape}, y shape: {y.shape}")

                # –õ–æ–≥–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
                unique, counts = np.unique(y, return_counts=True)
                self.log(f"Final class distribution in sequences:", 'info')
                for cls, cnt in zip(unique, counts):
                    percentage = (cnt / len(y)) * 100
                    self.log(f"  Class {cls}: {cnt} ({percentage:.1f}%)", 'info')

            return X, y

        except Exception as e:
            self.log(f"Error preparing features: {e}", 'error')
            return np.array([]), np.array([])

    def prepare_features_for_prediction(self, df: pd.DataFrame,
                                        lookback_window: int = 60,
                                        verbose: bool = True) -> np.ndarray:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏—á–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
        try:
            # –î–ª—è XGBoost –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ —Ñ–∏—á–∏ –∫—Ä–æ–º–µ —Ü–µ–ª–µ–≤—ã—Ö –∏ –±–∞–∑–æ–≤—ã—Ö OHLCV
            feature_columns = [col for col in df.columns
                               if not col.startswith('TARGET_')
                               and col not in ['open', 'high', 'low', 'close', 'volume']]

            # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
            available_features = [col for col in feature_columns if col in df.columns]

            if verbose:
                print(f"  üìä –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {len(available_features)} —Ñ–∏—á–µ–π")

            X_data = df[available_features].values

            # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ lookback_window –∑–Ω–∞—á–µ–Ω–∏–π
            if len(X_data) >= lookback_window:
                X_sequence = X_data[-lookback_window:].reshape(1, lookback_window, -1)

                if verbose:
                    print(f"  ‚úÖ –°–æ–∑–¥–∞–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {X_sequence.shape}")
                    print(f"  üî¢ –í—Å–µ–≥–æ —Ñ–∏—á–µ–π: {X_sequence.shape[1] * X_sequence.shape[2]}")

                return X_sequence
            else:
                if verbose:
                    print(f"  ‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
                return np.array([])

        except Exception as e:
            self.log(f"Error preparing features for prediction: {e}", 'error')
            return np.array([])

    def create_rolling_features(self, df: pd.DataFrame, window_sizes: List[int] = [5, 10, 20],
                                verbose: bool = True) -> pd.DataFrame:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —Å–∫–æ–ª—å–∑—è—â–∏—Ö —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫

        Args:
            df: –ò—Å—Ö–æ–¥–Ω—ã–π DataFrame
            window_sizes: –†–∞–∑–º–µ—Ä—ã –æ–∫–æ–Ω
            verbose: –§–ª–∞–≥ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

        Returns:
            DataFrame —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ñ–∏—á–∞–º–∏
        """
        try:
            data = df.copy()

            for window in window_sizes:
                # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è —Ü–µ–Ω—ã
                data[f'ROLLING_MEAN_{window}'] = data['close'].rolling(window=window).mean()
                data[f'ROLLING_STD_{window}'] = data['close'].rolling(window=window).std()
                data[f'ROLLING_MIN_{window}'] = data['low'].rolling(window=window).min()
                data[f'ROLLING_MAX_{window}'] = data['high'].rolling(window=window).max()

                # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –æ–±—ä–µ–º–∞
                data[f'VOLUME_MEAN_{window}'] = data['volume'].rolling(window=window).mean()
                data[f'VOLUME_STD_{window}'] = data['volume'].rolling(window=window).std()

                # –û—Ç–Ω–æ—à–µ–Ω–∏–µ —Ü–µ–Ω—ã –∫ —Å–∫–æ–ª—å–∑—è—â–∏–º —Å—Ä–µ–¥–Ω–∏–º
                data[f'PRICE_TO_MA_{window}'] = data['close'] / data[f'ROLLING_MEAN_{window}']

            if verbose:
                self.log(f"Added {len(window_sizes) * 7} rolling features")

            return data

        except Exception as e:
            self.log(f"Error creating rolling features: {e}", 'error')
            return df

    def normalize_features(self, X: np.ndarray, fit: bool = True,
                           scaler=None, verbose: bool = True) -> Tuple[np.ndarray, any]:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∏—á–µ–π

        Args:
            X: –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            fit: –§–ª–∞–≥ –æ–±—É—á–µ–Ω–∏—è —Å–∫–µ–π–ª–µ—Ä–∞
            scaler: –°—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Å–∫–µ–π–ª–µ—Ä
            verbose: –§–ª–∞–≥ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

        Returns:
            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Å–∫–µ–π–ª–µ—Ä
        """
        try:
            from sklearn.preprocessing import StandardScaler

            if fit or scaler is None:
                scaler = StandardScaler()
                # –î–ª—è 3D –¥–∞–Ω–Ω—ã—Ö (LSTM) –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ 2D –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–∫–µ–π–ª–µ—Ä–∞
                if len(X.shape) == 3:
                    X_2d = X.reshape(-1, X.shape[2])
                    scaler.fit(X_2d)
                    X_normalized = scaler.transform(X_2d).reshape(X.shape)
                else:
                    scaler.fit(X)
                    X_normalized = scaler.transform(X)
            else:
                if len(X.shape) == 3:
                    X_2d = X.reshape(-1, X.shape[2])
                    X_normalized = scaler.transform(X_2d).reshape(X.shape)
                else:
                    X_normalized = scaler.transform(X)

            if verbose:
                self.log(f"Data normalized. Shape: {X_normalized.shape}")

            return X_normalized, scaler

        except Exception as e:
            self.log(f"Error normalizing features: {e}", 'error')
            return X, scaler

    def split_train_test(self, X: np.ndarray, y: np.ndarray,
                         test_size: float = 0.2, verbose: bool = True) -> Tuple:
        """
        –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏

        Args:
            X: –ü—Ä–∏–∑–Ω–∞–∫–∏
            y: –¶–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            test_size: –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
            verbose: –§–ª–∞–≥ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

        Returns:
            X_train, X_test, y_train, y_test
        """
        try:
            split_idx = int(len(X) * (1 - test_size))

            X_train = X[:split_idx]
            X_test = X[split_idx:]
            y_train = y[:split_idx]
            y_test = y[split_idx:]

            if verbose:
                self.log(f"Data split: train={len(X_train)}, test={len(X_test)}")
                self.log(f"Train shape: {X_train.shape}, Test shape: {X_test.shape}")

                # –õ–æ–≥–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤ train/test
                unique_train, counts_train = np.unique(y_train, return_counts=True)
                unique_test, counts_test = np.unique(y_test, return_counts=True)

                self.log(f"Train class distribution:", 'info')
                for cls, cnt in zip(unique_train, counts_train):
                    percentage = (cnt / len(y_train)) * 100
                    self.log(f"  Class {cls}: {cnt} ({percentage:.1f}%)", 'info')

                self.log(f"Test class distribution:", 'info')
                for cls, cnt in zip(unique_test, counts_test):
                    percentage = (cnt / len(y_test)) * 100
                    self.log(f"  Class {cls}: {cnt} ({percentage:.1f}%)", 'info')

            return X_train, X_test, y_train, y_test

        except Exception as e:
            self.log(f"Error splitting data: {e}", 'error')
            return X, np.array([]), y, np.array([])

    def balance_classes(self, X: np.ndarray, y: np.ndarray, verbose: bool = True) -> Tuple[np.ndarray, np.ndarray]:
        """
        –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ –ø—É—Ç–µ–º oversampling –º–µ–Ω—å—à–∏—Ö –∫–ª–∞—Å—Å–æ–≤

        Args:
            X: –ü—Ä–∏–∑–Ω–∞–∫–∏ (3D –¥–ª—è LSTM)
            y: –ú–µ—Ç–∫–∏ (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å -1, 0, 1)
            verbose: –§–ª–∞–≥ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

        Returns:
            –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ X, y
        """
        try:
            unique, counts = np.unique(y, return_counts=True)

            if verbose:
                self.log(f"Original class distribution:", 'info')
                for cls, cnt in zip(unique, counts):
                    percentage = (cnt / len(y)) * 100
                    self.log(f"  Class {cls}: {cnt} samples ({percentage:.1f}%)", 'info')

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–µ–ª–µ–≤—ã–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
            # –¶–µ–ª—å: —Å–¥–µ–ª–∞—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–Ω–æ 40% HOLD, 30% LONG, 30% SHORT
            total_samples = len(y)
            target_counts = {
                -1: int(total_samples * 0.3),  # SHORT: 30%
                0: int(total_samples * 0.4),   # HOLD: 40%
                1: int(total_samples * 0.3)    # LONG: 30%
            }

            X_balanced = []
            y_balanced = []

            for cls in [-1, 0, 1]:
                # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –∫–ª–∞—Å—Å–∞
                idx = np.where(y == cls)[0]
                cls_count = len(idx)
                target_count = target_counts[cls]

                if verbose:
                    self.log(f"Processing class {cls}: {cls_count} samples -> target: {target_count}", 'info')

                if cls_count == 0:
                    self.log(f"Warning: No samples for class {cls}", 'warning')
                    continue

                if cls_count < target_count:
                    # Oversampling: –ø–æ–≤—Ç–æ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–∏–º–µ—Ä—ã
                    repeat_times = target_count // cls_count
                    remainder = target_count % cls_count

                    # –ü–æ–≤—Ç–æ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–∏–º–µ—Ä—ã
                    for _ in range(repeat_times):
                        X_balanced.append(X[idx])
                        y_balanced.append(y[idx])

                    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞—Ç–æ–∫ —Å–ª—É—á–∞–π–Ω–æ
                    if remainder > 0:
                        selected_idx = np.random.choice(idx, size=remainder, replace=True)
                        X_balanced.append(X[selected_idx])
                        y_balanced.append(y[selected_idx])

                    if verbose:
                        self.log(f"  Oversampled: {cls_count} -> {repeat_times * cls_count + remainder}", 'info')
                else:
                    # Undersampling: –±–µ—Ä–µ–º —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É
                    selected_idx = np.random.choice(idx, size=target_count, replace=False)
                    X_balanced.append(X[selected_idx])
                    y_balanced.append(y[selected_idx])

                    if verbose:
                        self.log(f"  Undersampled: {cls_count} -> {target_count}", 'info')

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º
            if X_balanced:
                X_balanced = np.concatenate(X_balanced, axis=0)
                y_balanced = np.concatenate(y_balanced, axis=0)
            else:
                X_balanced = X.copy()
                y_balanced = y.copy()

            # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º
            shuffle_idx = np.random.permutation(len(X_balanced))
            X_balanced = X_balanced[shuffle_idx]
            y_balanced = y_balanced[shuffle_idx]

            # –õ–æ–≥–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
            unique_bal, counts_bal = np.unique(y_balanced, return_counts=True)
            if verbose:
                self.log(f"Balanced class distribution:", 'info')
                for cls, cnt in zip(unique_bal, counts_bal):
                    percentage = (cnt / len(y_balanced)) * 100
                    self.log(f"  Class {cls}: {cnt} samples ({percentage:.1f}%)", 'info')

            return X_balanced, y_balanced

        except Exception as e:
            self.log(f"Error balancing classes: {e}", 'error')
            return X, y

    def add_advanced_features(self, df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö —Ñ–∏—á–µ–π –∫ –¥–∞–Ω–Ω—ã–º

        Args:
            df: DataFrame —Å –±–∞–∑–æ–≤—ã–º–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏
            verbose: –§–ª–∞–≥ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

        Returns:
            DataFrame —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ —Ñ–∏—á–∞–º–∏
        """
        try:
            if df.empty:
                return df

            data = df.copy()

            if verbose:
                self.log("Adding advanced features...", 'info')

            # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
            result = data.copy()

            # 1. –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏
            try:
                if 'RSI_14' in result.columns and 'MACD' in result.columns:
                    result['RSI_MACD_INTERACTION'] = result['RSI_14'] * result['MACD']
            except:
                pass

            try:
                if 'BB_WIDTH' in result.columns and 'ATR_14' in result.columns:
                    # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
                    result['BB_ATR_RATIO'] = result['BB_WIDTH'] / (result['ATR_14'].replace(0, np.nan) + 1e-6)
            except:
                pass

            try:
                if 'VOLUME_MA_20' in result.columns and 'volume' in result.columns:
                    result['VOLUME_RATIO'] = result['volume'] / (result['VOLUME_MA_20'].replace(0, np.nan) + 1e-6)
            except:
                pass

            # 2. –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            key_indicators = ['RSI_14', 'MACD', 'ATR_14', 'OBV']
            for indicator in key_indicators:
                if indicator in result.columns:
                    try:
                        # –°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ
                        result[f'{indicator}_MA_10'] = result[indicator].rolling(window=10, min_periods=3).mean()
                        result[f'{indicator}_MA_20'] = result[indicator].rolling(window=20, min_periods=5).mean()

                        # –°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ
                        result[f'{indicator}_STD_10'] = result[indicator].rolling(window=10, min_periods=3).std()

                        # Z-score (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ)
                        rolling_mean = result[indicator].rolling(window=20, min_periods=5).mean()
                        rolling_std = result[indicator].rolling(window=20, min_periods=5).std()
                        result[f'{indicator}_ZSCORE_20'] = (result[indicator] - rolling_mean) / (
                                    rolling_std.replace(0, np.nan) + 1e-6)
                    except:
                        continue

            # 3. –ú–æ–º–µ–Ω—Ç—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–ª—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π
            if 'close' in result.columns:
                try:
                    returns = result['close'].pct_change()
                    for window in [10, 20]:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º kurt() –≤–º–µ—Å—Ç–æ kurtosis()
                        result[f'RETURNS_SKEW_{window}'] = returns.rolling(window=window, min_periods=5).skew()
                        result[f'RETURNS_KURTOSIS_{window}'] = returns.rolling(window=window,
                                                                               min_periods=5).kurt()  # –ò–∑–º–µ–Ω–µ–Ω–æ —Å kurtosis()
                except Exception as e:
                    if verbose:
                        self.log(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ –º–æ–º–µ–Ω—Ç–æ–≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è: {e}", 'warning')

            # 4. –¶–µ–Ω–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–µ)
            if all(col in result.columns for col in ['high', 'low', 'close', 'open']):
                try:
                    # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –≤–Ω—É—Ç—Ä–∏ —Å–≤–µ—á–∏
                    result['CANDLE_BODY'] = abs(result['close'] - result['open'])
                    result['CANDLE_RANGE'] = result['high'] - result['low']
                    result['BODY_TO_RANGE_RATIO'] = result['CANDLE_BODY'] / (
                                result['CANDLE_RANGE'].replace(0, np.nan) + 1e-6)
                except:
                    pass

            # 5. Volume profile features (–±–µ–∑–æ–ø–∞—Å–Ω–∞—è –≤–µ—Ä—Å–∏—è)
            if 'volume' in result.columns and 'close' in result.columns:
                try:
                    # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ/—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (Accumulation/Distribution Line)
                    clv = ((result['close'] - result['low']) - (result['high'] - result['close'])) / (
                                result['high'] - result['low'])
                    clv = clv.replace([np.inf, -np.inf], np.nan).fillna(0)
                    result['ADL'] = (clv * result['volume']).cumsum()
                except:
                    pass

            # 6. –†–∞–∑–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ
            for window in [1, 2, 3, 5]:
                for indicator in ['RSI_14', 'MACD', 'ATR_14', 'close']:
                    if indicator in result.columns:
                        try:
                            result[f'{indicator}_DIFF_{window}'] = result[indicator].diff(window)
                        except:
                            pass

            # 7. –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏ (–µ—Å–ª–∏ –∏–Ω–¥–µ–∫—Å - datetime)
            try:
                if hasattr(result.index, 'hour'):
                    result['HOUR_OF_DAY'] = result.index.hour
                    result['DAY_OF_WEEK'] = result.index.dayofweek
                    result['DAY_OF_MONTH'] = result.index.day

                    # –¶–∏–∫–ª–∏—á–µ—Å–∫–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
                    result['HOUR_SIN'] = np.sin(2 * np.pi * result['HOUR_OF_DAY'] / 24)
                    result['HOUR_COS'] = np.cos(2 * np.pi * result['HOUR_OF_DAY'] / 24)
                    result['DAY_SIN'] = np.sin(2 * np.pi * result['DAY_OF_WEEK'] / 7)
                    result['DAY_COS'] = np.cos(2 * np.pi * result['DAY_OF_WEEK'] / 7)
            except:
                pass

            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –µ—Å–ª–∏ –æ–Ω–∏ —Å–æ–∑–¥–∞–≤–∞–ª–∏—Å—å
            temp_cols = ['CANDLE_BODY', 'CANDLE_RANGE']
            for col in temp_cols:
                if col in result.columns:
                    result = result.drop(columns=[col])

            # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –∑–Ω–∞—á–µ–Ω–∏—è–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—è–≤–∏–ª–∏—Å—å –∏–∑-–∑–∞ —Å–∫–æ–ª—å–∑—è—â–∏—Ö –æ–∫–æ–Ω
            initial_len = len(result)
            result = result.dropna()
            removed_count = initial_len - len(result)

            if verbose:
                self.log(f"Added {len(result.columns) - len(df.columns)} advanced features", 'info')
                self.log(f"Removed {removed_count} rows with NaN values from advanced features", 'info')
                self.log(f"Total features now: {len(result.columns)}", 'info')

                # –ü–æ–∫–∞–∑–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –Ω–æ–≤—ã—Ö —Ñ–∏—á–µ–π
                new_features = [col for col in result.columns if col not in df.columns]
                if new_features:
                    self.log(f"New features (first 10): {new_features[:10]}", 'debug')

            return result

        except Exception as e:
            self.log(f"Error adding advanced features: {e}", 'error')
            import traceback
            self.log(f"Traceback: {traceback.format_exc()}", 'error')
            return df