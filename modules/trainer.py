"""
–ú–æ–¥—É–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
"""

import numpy as np
import pandas as pd
import logging
import json
import os
import pickle
import hashlib
from datetime import datetime, timedelta
from typing import Dict, Tuple, Optional, Any, List
from config import config
from modules.database import Database
from modules.preprocessor import DataPreprocessor
from modules.state_manager import state_manager


# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential, load_model
    from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, BatchNormalization
    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.utils import to_categorical
except ImportError:
    print("Tensorflow/Keras not installed. LSTM features will be unavailable.")

try:
    import xgboost as xgb
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
    from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
    from sklearn.preprocessing import StandardScaler
    from sklearn.feature_selection import SelectKBest, f_classif, RFE
except ImportError:
    print("XGBoost/scikit-learn not installed. XGBoost features will be unavailable.")


class ModelTrainer:
    def __init__(self, verbose: bool = True):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞ –º–æ–¥–µ–ª–µ–π"""
        self.verbose = verbose
        self.setup_logging()
        self.db = Database(verbose=verbose)
        self.preprocessor = DataPreprocessor(verbose=verbose)
        self.model_cache = {}

    def setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        self.logger = logging.getLogger(__name__)
        if self.verbose:
            self.logger.setLevel(logging.INFO)
        else:
            self.logger.setLevel(logging.WARNING)

    def log(self, message: str, level: str = 'info'):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π"""
        if self.verbose:
            if level == 'info':
                self.logger.info(message)
            elif level == 'error':
                self.logger.error(message)
            elif level == 'warning':
                self.logger.warning(message)

    def generate_model_id(self, symbol: str, model_type: str) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ ID –¥–ª—è –º–æ–¥–µ–ª–∏
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        random_hash = hashlib.md5(f"{symbol}_{model_type}_{timestamp}".encode()).hexdigest()[:8]
        return f"{symbol}_{model_type}_{timestamp}_{random_hash}"

    def ensure_training_data(self, symbol: str, timeframe: str, training_days: int) -> bool:
        """
        –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –¥–æ—Å—Ç—É–ø–Ω—ã –∏–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã
        """
        try:
            from modules.data_fetcher import DataFetcher

            print(f"   üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è {symbol} ({timeframe})...")

            # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ state_manager
            train_start, train_end = state_manager.get_training_dates()
            days_back = max(training_days, (train_end - train_start).days)

            print(f"   üìÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {days_back} –¥–Ω–µ–π...")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑–µ
            existing_data = self.db.get_historical_data(
                symbol=symbol,
                timeframe=timeframe,
                start_date=train_start,
                end_date=train_end,
                verbose=False
            )

            min_samples_needed = config.model.LOOKBACK_WINDOW * 10  # –ú–∏–Ω–∏–º—É–º 10 –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
            if len(existing_data) >= min_samples_needed:
                print(f"   ‚úÖ –î–∞–Ω–Ω—ã–µ —É–∂–µ –µ—Å—Ç—å: {len(existing_data)} —Å–≤–µ—á–µ–π")
                return True

            print(f"   ‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {len(existing_data)} –∏–∑ {min_samples_needed} –Ω—É–∂–Ω—ã—Ö")
            print(f"   üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            data_fetcher = DataFetcher()
            data = data_fetcher.fetch_historical_data(
                symbol=symbol,
                timeframe=timeframe,
                days_back=days_back
            )

            if data.empty:
                print(f"   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è {symbol}")
                return False

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É
            success = self.db.store_historical_data(
                symbol=symbol,
                timeframe=timeframe,
                data=data,
                verbose=True
            )

            if success:
                print(f"   ‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {len(data)} —Å–≤–µ—á–µ–π")
                return True
            else:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö")
                return False

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            import traceback
            traceback.print_exc()
            return False

    def prepare_training_data(self, symbol: str, timeframe: str,
                              use_advanced_features: bool = True,
                              verbose: bool = True) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç X, y –∏ —Å–ø–∏—Å–æ–∫ —Ñ–∏—á–µ–π
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ state_manager
            train_start, train_end = state_manager.get_training_dates()

            print(f"   üìÖ –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å {train_start.date()} –ø–æ {train_end.date()}...")

            # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            data = self.db.get_historical_data(
                symbol=symbol,
                timeframe=timeframe,
                start_date=train_start,
                end_date=train_end,
                verbose=verbose
            )

            if data.empty:
                print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞")
                return np.array([]), np.array([]), []

            print(f"   ‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(data)} —Å–≤–µ—á–µ–π")

            # –†–∞—Å—á–µ—Ç –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            print(f"   üìä –†–∞—Å—á–µ—Ç –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤...")
            data_with_indicators = self.preprocessor.calculate_all_indicators(
                data, verbose=verbose
            )

            if data_with_indicators.empty:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã")
                return np.array([]), np.array([]), []

            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if use_advanced_features:
                print(f"   üîß –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö —Ñ–∏—á...")
                data_with_indicators = self.preprocessor.add_advanced_features(
                    data_with_indicators, verbose=verbose
                )

            print(f"   ‚úÖ –í—Å–µ–≥–æ —Ñ–∏—á–µ–π: {len(data_with_indicators.columns)}")

            # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∏—á–µ–π –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            all_features = list(data_with_indicators.columns)

            # –ò–°–ö–õ–Æ–ß–ê–ï–ú –í–†–ï–ú–ï–ù–ù–´–ï –§–ò–ß–ò –ò –¶–ï–õ–ï–í–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            exclude_patterns = ['TARGET_', 'HOUR', 'DAY_OF_WEEK', 'MONTH', 'WEEK', '_SIN', '_COS']

            feature_columns_for_training = []
            for feature in all_features:
                exclude = False
                for pattern in exclude_patterns:
                    if pattern in feature:
                        exclude = True
                        break
                if not exclude:
                    feature_columns_for_training.append(feature)

            print(f"   üìã –§–∏—á–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(feature_columns_for_training)}")
            if verbose and len(feature_columns_for_training) <= 20:
                print(f"   üìã –°–ø–∏—Å–æ–∫ —Ñ–∏—á–µ–π: {feature_columns_for_training}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫–∏–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –µ—Å—Ç—å
            target_columns = [col for col in data_with_indicators.columns if col.startswith('TARGET_')]

            if not target_columns:
                print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ")
                return np.array([]), np.array([]), []

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º TARGET_CLASS_5 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            target_column = 'TARGET_CLASS_5'
            if target_column not in data_with_indicators.columns:
                target_column = target_columns[0]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é

            print(f"   üéØ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {target_column}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
            if target_column in data_with_indicators.columns:
                class_dist = data_with_indicators[target_column].value_counts()
                print(f"   üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
                for cls, count in class_dist.items():
                    percentage = (count / len(data_with_indicators)) * 100
                    print(f"      –ö–ª–∞—Å—Å {cls}: {count} ({percentage:.1f}%)")

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
                min_class = class_dist.min()
                max_class = class_dist.max()
                if min_class > 0:
                    imbalance_ratio = max_class / min_class
                    if imbalance_ratio > 3:
                        print(f"   ‚ö†Ô∏è  –°–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ {imbalance_ratio:.1f}:1")

            # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
            initial_len = len(data_with_indicators)
            data_with_indicators = data_with_indicators.dropna(subset=[target_column])
            if len(data_with_indicators) < initial_len:
                print(f"   üßπ –£–¥–∞–ª–µ–Ω–æ {initial_len - len(data_with_indicators)} —Å—Ç—Ä–æ–∫ —Å NaN –≤ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π")

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            print(f"   üîÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π...")
            X, y, feature_names = self.prepare_sequences_with_features(
                df=data_with_indicators,
                target_column=target_column,
                lookback_window=config.model.LOOKBACK_WINDOW,
                use_advanced_features=use_advanced_features,
                feature_columns=feature_columns_for_training,  # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ø–µ—Ä–µ–¥–∞–µ–º —Ñ–∏—á–∏
                verbose=verbose
            )

            if len(X) == 0 or len(y) == 0:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                return np.array([]), np.array([]), []

            print(f"   ‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(X)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
            print(f"   üìê –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å X: {X.shape}")
            print(f"   üìê –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å y: {y.shape}")
            print(f"   üî§ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏—á–µ–π: {len(feature_names)}")

            # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏—á–∏ –≤ preprocessor –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            self.preprocessor.last_training_features = feature_names.copy()
            print(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(feature_names)} —Ñ–∏—á–µ–π –≤ preprocessor")

            return X, y, feature_names

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            import traceback
            traceback.print_exc()
            return np.array([]), np.array([]), []

    def train_lstm_classifier(self, symbol: str, timeframe: str = '5m',
                            use_advanced_features: bool = True,
                            verbose: bool = True) -> Dict[str, Any]:
        """
        –û–±—É—á–µ–Ω–∏–µ LSTM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        """
        try:
            print(f"\nüöÄ –ù–ê–ß–ò–ù–ê–ï–ú –û–ë–£–ß–ï–ù–ò–ï LSTM –ú–û–î–ï–õ–ò")
            print(f"   –ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞: {symbol}")
            print(f"   –¢–∞–π–º—Ñ—Ä–µ–π–º: {timeframe}")
            print(f"   –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏: {'–î–∞' if use_advanced_features else '–ù–µ—Ç'}")
            print("=" * 70)

            # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            training_days = state_manager.get_training_period()
            if not self.ensure_training_data(symbol, timeframe, training_days):
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–µ—Å–ø–µ—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LSTM")
                return {'model': None, 'metrics': {}, 'feature_importance': None}

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X, y, feature_names = self.prepare_training_data(
                symbol, timeframe, use_advanced_features, verbose
            )

            if len(X) == 0:
                print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                return {'model': None, 'metrics': {}, 'feature_importance': None}

            print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã: {len(X)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")

            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            y_categorical = y + 1  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º [-1, 0, 1] -> [0, 1, 2]
            y_categorical = to_categorical(y_categorical, num_classes=3)

            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/validation
            X_train, X_val, y_train, y_val = train_test_split(
                X, y_categorical, test_size=0.2, random_state=42, stratify=y_categorical
            )

            print(f"üìà –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
            print(f"   –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
            print(f"   –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_val)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            print(f"üî¢ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
            X_train_norm, scaler = self.preprocessor.normalize_features(
                X_train, fit=True, verbose=verbose
            )
            X_val_norm, _ = self.preprocessor.normalize_features(
                X_val, fit=False, scaler=scaler, verbose=verbose
            )

            # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ LSTM
            print(f"üèóÔ∏è  –°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã LSTM...")
            input_shape = (X_train_norm.shape[1], X_train_norm.shape[2])

            model = Sequential([
                Input(shape=input_shape),
                LSTM(config.model.LSTM_UNITS[0], return_sequences=True,
                     dropout=config.model.LSTM_DROPOUT, recurrent_dropout=config.model.LSTM_DROPOUT),
                BatchNormalization(),
                LSTM(config.model.LSTM_UNITS[1], return_sequences=True,
                     dropout=config.model.LSTM_DROPOUT, recurrent_dropout=config.model.LSTM_DROPOUT),
                BatchNormalization(),
                LSTM(config.model.LSTM_UNITS[2], dropout=config.model.LSTM_DROPOUT),
                BatchNormalization(),
                Dense(128, activation='relu'),
                Dropout(0.4),
                Dense(64, activation='relu'),
                Dropout(0.3),
                Dense(32, activation='relu'),
                Dropout(0.2),
                Dense(3, activation='softmax')
            ])

            # –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏
            print(f"‚öôÔ∏è  –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏...")
            model.compile(
                optimizer=Adam(learning_rate=config.model.LSTM_LEARNING_RATE),
                loss='categorical_crossentropy',
                metrics=['accuracy', tf.keras.metrics.Precision(name='precision'),
                        tf.keras.metrics.Recall(name='recall'),
                        tf.keras.metrics.AUC(name='auc')]
            )

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏
            print(f"\nüèõÔ∏è  –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:")
            model.summary(print_fn=lambda x: print(f"   {x}"))

            # Callbacks
            print(f"\n‚è±Ô∏è  –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
            print(f"   –≠–ø–æ—Ö: {config.model.LSTM_EPOCHS}")
            print(f"   –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {config.model.LSTM_BATCH_SIZE}")
            print(f"   Learning rate: {config.model.LSTM_LEARNING_RATE}")
            print(f"   Early stopping patience: {config.model.LSTM_PATIENCE}")

            # –°–æ–∑–¥–∞–µ–º –∫–∞—Ç–∞–ª–æ–≥ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è TensorBoard
            log_dir = os.path.join(config.LOG_DIR, "tensorboard", datetime.now().strftime("%Y%m%d-%H%M%S"))
            os.makedirs(log_dir, exist_ok=True)

            callbacks = [
                EarlyStopping(
                    monitor='val_loss',
                    patience=config.model.LSTM_PATIENCE,
                    restore_best_weights=True,
                    verbose=1
                ),
                ReduceLROnPlateau(
                    monitor='val_loss',
                    factor=0.5,
                    patience=5,
                    min_lr=0.00001,
                    verbose=1
                ),
                ModelCheckpoint(
                    filepath=os.path.join(config.MODEL_DIR, f"lstm_best_{symbol}_{timeframe}.h5"),
                    monitor='val_accuracy',
                    save_best_only=True,
                    verbose=0
                )
            ]

            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            print(f"\nüéì –û–±—É—á–µ–Ω–∏–µ –Ω–∞—á–∞–ª–æ—Å—å:")
            print("=" * 70)

            history = model.fit(
                X_train_norm, y_train,
                epochs=config.model.LSTM_EPOCHS,
                batch_size=config.model.LSTM_BATCH_SIZE,
                validation_data=(X_val_norm, y_val),
                callbacks=callbacks,
                verbose=1
            )

            print("=" * 70)
            print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö: {len(history.history['loss'])}")

            # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
            print(f"\nüìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ...")
            eval_results = model.evaluate(X_val_norm, y_val, verbose=0)

            # –†–∞—Å—á–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
            y_pred_proba = model.predict(X_val_norm, verbose=0)
            y_pred = np.argmax(y_pred_proba, axis=1)
            y_true = np.argmax(y_val, axis=1)
            y_pred_original = y_pred - 1
            y_true_original = y_true - 1

            # –ü–æ–¥—Ä–æ–±–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            accuracy = accuracy_score(y_true_original, y_pred_original)
            precision = precision_score(y_true_original, y_pred_original, average='weighted')
            recall = recall_score(y_true_original, y_pred_original, average='weighted')
            f1 = f1_score(y_true_original, y_pred_original, average='weighted')
            conf_matrix = confusion_matrix(y_true_original, y_pred_original)

            # Classification report
            class_report = classification_report(y_true_original, y_pred_original,
                                                target_names=['DOWN', 'HOLD', 'UP'],
                                                output_dict=True)

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç—Ä–∏–∫
            metrics = {
                'val_loss': float(eval_results[0]),
                'val_accuracy': float(eval_results[1]),
                'val_precision': float(eval_results[2]),
                'val_recall': float(eval_results[3]),
                'val_auc': float(eval_results[4]),
                'accuracy': float(accuracy),
                'precision': float(precision),
                'recall': float(recall),
                'f1_score': float(f1),
                'confusion_matrix': conf_matrix.tolist(),
                'classification_report': class_report,
                'training_samples': len(X_train),
                'validation_samples': len(X_val),
                'feature_count': X_train_norm.shape[2],
                'sequence_length': X_train_norm.shape[1],
                'training_period': {
                    'start': state_manager.get_training_dates()[0].isoformat(),
                    'end': state_manager.get_training_dates()[1].isoformat()
                },
                'training_history': {
                    'loss': [float(x) for x in history.history.get('loss', [])],
                    'val_loss': [float(x) for x in history.history.get('val_loss', [])],
                    'accuracy': [float(x) for x in history.history.get('accuracy', [])],
                    'val_accuracy': [float(x) for x in history.history.get('val_accuracy', [])]
                }
            }

            print(f"\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–£–ß–ï–ù–ò–Ø LSTM:")
            print(f"   Validation Loss: {metrics['val_loss']:.4f}")
            print(f"   Validation Accuracy: {metrics['val_accuracy']:.4f}")
            print(f"   Accuracy: {metrics['accuracy']:.4f}")
            print(f"   Precision: {metrics['precision']:.4f}")
            print(f"   Recall: {metrics['recall']:.4f}")
            print(f"   F1 Score: {metrics['f1_score']:.4f}")
            print(f"   AUC: {metrics['val_auc']:.4f}")

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º confusion matrix
            print(f"\nüìä CONFUSION MATRIX:")
            print("   DOWN  HOLD  UP")
            for i, row in enumerate(conf_matrix):
                class_name = ['DOWN', 'HOLD', 'UP'][i]
                print(f"   {class_name} {row}")

            # Feature importance –¥–ª—è LSTM (—Å—Ä–µ–¥–Ω–∏–µ –≤–µ—Å–∞)
            feature_importance = None
            try:
                # –ü—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–± –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ñ–∏—á–µ–π –¥–ª—è LSTM
                layer_weights = []
                for layer in model.layers:
                    if isinstance(layer, LSTM):
                        layer_weights.append(layer.get_weights()[0])  # –í–µ—Å–∞ —è—á–µ–µ–∫

                if layer_weights:
                    avg_weights = np.mean([np.mean(np.abs(w), axis=1) for w in layer_weights], axis=0)
                    if len(avg_weights) == len(feature_names):
                        feature_importance = dict(zip(feature_names, avg_weights.tolist()))
                        print(f"\nüìà –†–∞—Å—Å—á–∏—Ç–∞–Ω–∞ –≤–∞–∂–Ω–æ—Å—Ç—å —Ñ–∏—á–µ–π –¥–ª—è LSTM")
            except:
                pass

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫–µ–π–ª–µ—Ä –≤ –∞—Ç—Ä–∏–±—É—Ç–µ –º–æ–¥–µ–ª–∏
            model.scaler = scaler
            model.feature_names = feature_names

            return {
                'model': model,
                'metrics': metrics,
                'feature_importance': feature_importance,
                'feature_names': feature_names,
                'scaler': scaler
            }

        except Exception as e:
            print(f"\n‚ùå –û–®–ò–ë–ö–ê –û–ë–£–ß–ï–ù–ò–Ø LSTM: {e}")
            import traceback
            traceback.print_exc()
            return {'model': None, 'metrics': {}, 'feature_importance': None}

    def train_xgboost_classifier(self, symbol: str, timeframe: str = '5m',
                                 use_advanced_features: bool = True,
                                 verbose: bool = True) -> Dict[str, Any]:
        """
        –û–±—É—á–µ–Ω–∏–µ XGBoost –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        """
        try:
            print(f"\nüöÄ –ù–ê–ß–ò–ù–ê–ï–ú –û–ë–£–ß–ï–ù–ò–ï XGBOOST –ú–û–î–ï–õ–ò")
            print(f"   –ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞: {symbol}")
            print(f"   –¢–∞–π–º—Ñ—Ä–µ–π–º: {timeframe}")
            print(f"   –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏: {'–î–∞' if use_advanced_features else '–ù–µ—Ç'}")
            print("=" * 70)

            # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            training_days = state_manager.get_training_period()
            if not self.ensure_training_data(symbol, timeframe, training_days):
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–µ—Å–ø–µ—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è XGBoost")
                return {'model': None, 'metrics': {}, 'feature_importance': None}

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X, y, feature_names = self.prepare_training_data(
                symbol, timeframe, use_advanced_features, verbose
            )

            if len(X) == 0:
                print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                return {'model': None, 'metrics': {}, 'feature_importance': None}

            print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã: {len(X)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
            print(f"   –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ 3D -> 2D –¥–ª—è XGBoost...")

            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ 3D -> 2D –¥–ª—è XGBoost
            X_2d = X.reshape(X.shape[0], -1)
            y_xgb = y + 1  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º [-1, 0, 1] -> [0, 1, 2]

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ –∏–º–µ–Ω–∞ —Ñ–∏—á–µ–π –¥–ª—è 2D –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
            expanded_feature_names = []
            for i in range(X.shape[1]):  # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —à–∞–≥–∞
                for feature_name in feature_names:
                    expanded_feature_names.append(f"{feature_name}_t-{X.shape[1] - i - 1}")

            print(f"   üìê –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å X_2d: {X_2d.shape}")
            print(f"   üî§ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏—á–µ–π –≤ 2D: {len(expanded_feature_names)}")

            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/validation
            X_train, X_val, y_train, y_val = train_test_split(
                X_2d, y_xgb, test_size=0.2, random_state=42, stratify=y_xgb
            )

            print(f"üìà –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
            print(f"   –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)} –æ–±—Ä–∞–∑—Ü–æ–≤")
            print(f"   –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_val)} –æ–±—Ä–∞–∑—Ü–æ–≤")

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            print(f"üî¢ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
            X_train_norm, scaler = self.preprocessor.normalize_features(
                X_train, fit=True, verbose=verbose
            )
            X_val_norm, _ = self.preprocessor.normalize_features(
                X_val, fit=False, scaler=scaler, verbose=verbose
            )

            # –í–ê–ñ–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∏—á–∞—Ö –≤ —Å–∫–µ–π–ª–µ—Ä–µ
            if hasattr(scaler, 'feature_names_in_'):
                scaler.feature_names_in_ = expanded_feature_names
            elif hasattr(scaler, 'feature_names'):
                scaler.feature_names = expanded_feature_names

            # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ XGBoost –º–æ–¥–µ–ª–∏
            print(f"\nüå≤ –°–æ–∑–¥–∞–Ω–∏–µ XGBoost –º–æ–¥–µ–ª–∏...")
            print(f"   n_estimators: {config.model.XGB_N_ESTIMATORS}")
            print(f"   max_depth: {config.model.XGB_MAX_DEPTH}")
            print(f"   learning_rate: {config.model.XGB_LEARNING_RATE}")
            print(f"   subsample: {config.model.XGB_SUBSAMPLE}")
            print(f"   colsample_bytree: {config.model.XGB_COLSAMPLE_BYTREE}")
            print(f"   early_stopping_rounds: {config.model.XGB_EARLY_STOPPING_ROUNDS}")

            model = xgb.XGBClassifier(
                n_estimators=config.model.XGB_N_ESTIMATORS,
                max_depth=config.model.XGB_MAX_DEPTH,
                learning_rate=config.model.XGB_LEARNING_RATE,
                subsample=config.model.XGB_SUBSAMPLE,
                colsample_bytree=config.model.XGB_COLSAMPLE_BYTREE,
                objective='multi:softprob',
                num_class=3,
                random_state=42,
                n_jobs=-1,
                verbosity=0,
                enable_categorical=False,
                tree_method='hist',
                eval_metric=['merror', 'mlogloss'],
                early_stopping_rounds=config.model.XGB_EARLY_STOPPING_ROUNDS
            )

            # –û–±—É—á–µ–Ω–∏–µ —Å early stopping
            print(f"\nüéì –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ XGBoost...")

            eval_set = [(X_train_norm, y_train), (X_val_norm, y_val)]
            eval_metric = ["merror", "mlogloss"]

            model.fit(
                X_train_norm, y_train,
                eval_set=eval_set,
                verbose=10
            )

            # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
            print(f"\nüìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏...")
            y_pred = model.predict(X_val_norm)
            y_pred_proba = model.predict_proba(X_val_norm)
            y_pred_original = y_pred - 1
            y_val_original = y_val - 1

            accuracy = accuracy_score(y_val_original, y_pred_original)
            precision = precision_score(y_val_original, y_pred_original, average='weighted')
            recall = recall_score(y_val_original, y_pred_original, average='weighted')
            f1 = f1_score(y_val_original, y_pred_original, average='weighted')
            conf_matrix = confusion_matrix(y_val_original, y_pred_original)

            # Classification report
            class_report = classification_report(y_val_original, y_pred_original,
                                                 target_names=['DOWN', 'HOLD', 'UP'],
                                                 output_dict=True)

            # Feature importance
            feature_importance_dict = {}
            if hasattr(model, 'feature_importances_'):
                importance_values = model.feature_importances_
                if len(importance_values) == len(expanded_feature_names):
                    for i, feature_name in enumerate(expanded_feature_names):
                        feature_importance_dict[feature_name] = float(importance_values[i])

                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –æ—Å–Ω–æ–≤–Ω—ã–º —Ñ–∏—á–∞–º (–±–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ª–∞–≥–æ–≤)
                aggregated_importance = {}
                for feature_name, importance in feature_importance_dict.items():
                    base_feature = feature_name.split('_t-')[0]  # –£–±–∏—Ä–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–∞–≥
                    aggregated_importance[base_feature] = aggregated_importance.get(base_feature, 0) + importance

                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
                sorted_features = sorted(aggregated_importance.items(), key=lambda x: x[1], reverse=True)
                feature_importance_dict = dict(sorted_features[:20])  # –¢–æ–ø-20 —Ñ–∏—á–µ–π

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç—Ä–∏–∫
            metrics = {
                'accuracy': float(accuracy),
                'precision': float(precision),
                'recall': float(recall),
                'f1_score': float(f1),
                'confusion_matrix': conf_matrix.tolist(),
                'classification_report': class_report,
                'training_samples': len(X_train),
                'validation_samples': len(X_val),
                'best_iteration': int(model.best_iteration) if hasattr(model,
                                                                       'best_iteration') else config.model.XGB_N_ESTIMATORS,
                'feature_count': X_train_norm.shape[1],
                'training_period': {
                    'start': state_manager.get_training_dates()[0].isoformat(),
                    'end': state_manager.get_training_dates()[1].isoformat()
                },
                'eval_results': {
                    'train_merror': model.evals_result()['validation_0']['merror'][-1] if hasattr(model,
                                                                                                  'evals_result') else 0,
                    'train_mlogloss': model.evals_result()['validation_0']['mlogloss'][-1] if hasattr(model,
                                                                                                      'evals_result') else 0,
                    'val_merror': model.evals_result()['validation_1']['merror'][-1] if hasattr(model,
                                                                                                'evals_result') else 0,
                    'val_mlogloss': model.evals_result()['validation_1']['mlogloss'][-1] if hasattr(model,
                                                                                                    'evals_result') else 0
                }
            }

            print(f"\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–£–ß–ï–ù–ò–Ø XGBOOST:")
            print(f"   Accuracy: {accuracy:.4f}")
            print(f"   Precision: {precision:.4f}")
            print(f"   Recall: {recall:.4f}")
            print(f"   F1 Score: {f1:.4f}")
            print(f"   Best iteration: {metrics['best_iteration']}")

            if 'eval_results' in metrics:
                print(f"   Train merror: {metrics['eval_results']['train_merror']:.4f}")
                print(f"   Validation merror: {metrics['eval_results']['val_merror']:.4f}")

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º confusion matrix
            print(f"\nüìä CONFUSION MATRIX:")
            print("   DOWN  HOLD  UP")
            for i, row in enumerate(conf_matrix):
                class_name = ['DOWN', 'HOLD', 'UP'][i]
                print(f"   {class_name} {row}")

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø —Ñ–∏—á–µ–π –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
            if feature_importance_dict:
                print(f"\nüèÜ –¢–û–ü-10 –í–ê–ñ–ù–ï–ô–®–ò–• –§–ò–ß:")
                top_features = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)[:10]
                for i, (feature, importance) in enumerate(top_features, 1):
                    print(f"   {i:2d}. {feature:<30} {importance:.4f}")

            # –í–ê–ñ–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–æ—Ö—Ä–∞–Ω—è–µ–º –ü–†–ê–í–ò–õ–¨–ù–´–ï —Ñ–∏—á–∏ –≤ –∞—Ç—Ä–∏–±—É—Ç–∞—Ö –º–æ–¥–µ–ª–∏
            model.scaler = scaler
            model.feature_names = expanded_feature_names  # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏ (1500), –∞ –Ω–µ –±–∞–∑–æ–≤—ã–µ
            model.expanded_feature_names = expanded_feature_names  # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏ (1500)
            model.base_feature_names = feature_names  # –ë–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏ (25) - –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            model._features = expanded_feature_names  # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ

            # –î–û–ë–ê–í–õ–Ø–ï–ú –ö–†–ò–¢–ò–ß–ï–°–ö–£–Æ –ò–ù–§–û–†–ú–ê–¶–ò–Æ:
            model._lookback_window = config.model.LOOKBACK_WINDOW
            model._base_features_count = len(feature_names)
            model._expanded_features_count = len(expanded_feature_names)
            model._model_type = 'xgb_class'  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏

            print(f"\nüíæ –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –§–ò–ß–ê–• –î–õ–Ø –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø:")
            print(f"   –ë–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏: {len(feature_names)}")
            print(f"   –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏ (2D): {len(expanded_feature_names)}")
            print(f"   Lookback window: {config.model.LOOKBACK_WINDOW}")
            print(f"   –§–æ—Ä–º—É–ª–∞: {len(feature_names)} √ó {config.model.LOOKBACK_WINDOW} = {len(expanded_feature_names)}")
            print(f"   üìã –ü–µ—Ä–≤—ã–µ 10 —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö —Ñ–∏—á–µ–π: {expanded_feature_names[:10]}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ formula —Ä–∞–±–æ—Ç–∞–µ—Ç
            expected_expanded = len(feature_names) * config.model.LOOKBACK_WINDOW
            if expected_expanded != len(expanded_feature_names):
                print(f"  ‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –§–æ—Ä–º—É–ª–∞ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç!")
                print(f"     –û–∂–∏–¥–∞–ª–æ—Å—å: {expected_expanded}, –ø–æ–ª—É—á–∏–ª–æ—Å—å: {len(expanded_feature_names)}")

            return {
                'model': model,
                'metrics': metrics,
                'feature_importance': feature_importance_dict,
                'feature_names': expanded_feature_names,  # –¢–µ–ø–µ—Ä—å –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏
                'base_feature_names': feature_names,
                'expanded_feature_names': expanded_feature_names,
                'scaler': scaler
            }

        except Exception as e:
            print(f"\n‚ùå –û–®–ò–ë–ö–ê –û–ë–£–ß–ï–ù–ò–Ø XGBOOST: {e}")
            import traceback
            traceback.print_exc()
            return {'model': None, 'metrics': {}, 'feature_importance': None}

    def compare_models(self, comparison_results: Dict[str, Dict]):
        """
        –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        """
        print(f"\nüî¨ –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô:")
        print("=" * 80)

        if not comparison_results:
            print("   ‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
            return

        print(f"{'–ú–æ–¥–µ–ª—å':<15} {'Accuracy':<10} {'F1 Score':<10} {'Precision':<10} {'Recall':<10} {'Samples':<10}")
        print("-" * 80)

        best_model = None
        best_score = -1

        for model_name, results in comparison_results.items():
            metrics = results.get('metrics', {})
            accuracy = metrics.get('accuracy', metrics.get('val_accuracy', 0))
            f1 = metrics.get('f1_score', 0)
            precision = metrics.get('precision', metrics.get('val_precision', 0))
            recall = metrics.get('recall', metrics.get('val_recall', 0))
            samples = metrics.get('training_samples', 0) + metrics.get('validation_samples', 0)

            print(f"{model_name:<15} {accuracy:.4f}      {f1:.4f}      {precision:.4f}      {recall:.4f}      {samples:<10}")

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ F1 score
            if f1 > best_score:
                best_score = f1
                best_model = model_name

        print("-" * 80)
        print(f"üèÜ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model} (F1 Score: {best_score:.4f})")

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        if best_score > 0.6:
            print(f"   ‚úÖ –û—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã! –ú–æ–¥–µ–ª—å {best_model} –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å")
        elif best_score > 0.5:
            print(f"   üëç –•–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å {best_model} –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏")
        else:
            print(f"   ‚ö†Ô∏è  –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∏–∂–µ —Å—Ä–µ–¥–Ω–µ–≥–æ, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è:")
            print(f"      1. –£–≤–µ–ª–∏—á–∏—Ç—å –ø–µ—Ä–∏–æ–¥ –æ–±—É—á–µ–Ω–∏—è")
            print(f"      2. –î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤")
            print(f"      3. –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–µ–π")

    def save_model(self, model: Any, model_id: str, symbol: str,
                   model_type: str, metrics: Dict,
                   feature_importance: Dict = None,
                   verbose: bool = True) -> bool:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        """
        try:
            # –ü—É—Ç–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            model_filename = f"{model_id}.h5" if 'lstm' in model_type else f"{model_id}.pkl"
            model_path = os.path.join(config.MODEL_DIR, model_filename)

            scaler_filename = f"{model_id}_scaler.pkl"
            scaler_path = os.path.join(config.MODEL_DIR, scaler_filename)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            if 'lstm' in model_type:
                model.save(model_path)
            elif 'xgb' in model_type:
                with open(model_path, 'wb') as f:
                    pickle.dump(model, f)
            else:
                self.log(f"Unknown model type: {model_type}", 'error')
                return False

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–∫–µ–π–ª–µ—Ä–∞
            if hasattr(model, 'scaler') and model.scaler is not None:
                with open(scaler_path, 'wb') as f:
                    pickle.dump(model.scaler, f)
            else:
                # –ï—Å–ª–∏ —Å–∫–µ–π–ª–µ—Ä–∞ –Ω–µ—Ç, —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª
                with open(scaler_path, 'wb') as f:
                    pickle.dump(None, f)

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            parameters = {
                'lookback_window': config.model.LOOKBACK_WINDOW,
                'prediction_horizon': config.model.PREDICTION_HORIZON,
                'timeframe': state_manager.get_selected_timeframe(),
                'training_period': {
                    'days': state_manager.get_training_period(),
                    'start': state_manager.get_training_dates()[0].isoformat(),
                    'end': state_manager.get_training_dates()[1].isoformat()
                },
                'feature_count': metrics.get('feature_count', 0),
                'use_advanced_features': True  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤–∫–ª—é—á–µ–Ω–æ
            }

            if 'lstm' in model_type:
                parameters.update({
                    'lstm_units': config.model.LSTM_UNITS,
                    'lstm_dropout': config.model.LSTM_DROPOUT,
                    'lstm_learning_rate': config.model.LSTM_LEARNING_RATE,
                    'epochs': config.model.LSTM_EPOCHS,
                    'batch_size': config.model.LSTM_BATCH_SIZE,
                    'patience': config.model.LSTM_PATIENCE
                })
            elif 'xgb' in model_type:
                parameters.update({
                    'max_depth': config.model.XGB_MAX_DEPTH,
                    'learning_rate': config.model.XGB_LEARNING_RATE,
                    'n_estimators': config.model.XGB_N_ESTIMATORS,
                    'subsample': config.model.XGB_SUBSAMPLE,
                    'colsample_bytree': config.model.XGB_COLSAMPLE_BYTREE,
                    'early_stopping_rounds': config.model.XGB_EARLY_STOPPING_ROUNDS
                })

            # –î–æ–±–∞–≤–ª—è–µ–º feature importance –≤ –º–µ—Ç—Ä–∏–∫–∏
            if feature_importance:
                metrics['feature_importance'] = feature_importance

            # –í–ê–ñ–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ñ–∏—á–∏ –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö
            # –î–ª—è XGBoost —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏
            if 'xgb' in model_type:
                if hasattr(model, 'expanded_feature_names'):
                    metrics['feature_names'] = model.expanded_feature_names
                    metrics['base_feature_names'] = model.base_feature_names if hasattr(model,
                                                                                        'base_feature_names') else []
                    if verbose:
                        print(f"  üíæ –î–ª—è XGBoost —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏: {len(model.expanded_feature_names)} —Ñ–∏—á–µ–π")
                        print(
                            f"  üíæ –ë–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏: {len(model.base_feature_names) if hasattr(model, 'base_feature_names') else 0} —Ñ–∏—á–µ–π")
                elif hasattr(model, 'feature_names'):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ª–∏ —ç—Ç–æ —Ñ–∏—á–∏
                    if len(model.feature_names) > 100:  # –ú–Ω–æ–≥–æ —Ñ–∏—á–µ–π = —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ
                        metrics['feature_names'] = model.feature_names
                        metrics['base_feature_names'] = model.base_feature_names if hasattr(model,
                                                                                            'base_feature_names') else []
                    else:
                        # –ë–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏ - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                        metrics['feature_names'] = model.feature_names
                        if verbose:
                            print(f"  ‚ö†Ô∏è  –î–ª—è XGBoost —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –±–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏: {len(model.feature_names)} —Ñ–∏—á–µ–π")
            else:
                # –î–ª—è LSTM —Å–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏
                if hasattr(model, 'feature_names'):
                    metrics['feature_names'] = model.feature_names

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            success = self.db.save_model_info(
                model_id=model_id,
                symbol=symbol,
                timeframe=state_manager.get_selected_timeframe(),
                model_type=model_type,
                parameters=json.dumps(parameters),
                metrics=json.dumps(metrics),
                model_path=model_path,
                feature_importance=json.dumps(feature_importance) if feature_importance else None,
                verbose=verbose
            )

            if success:
                if verbose:
                    print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_id} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                    print(f"   –§–∞–π–ª –º–æ–¥–µ–ª–∏: {model_path}")
                    print(f"   –§–∞–π–ª —Å–∫–µ–π–ª–µ—Ä–∞: {scaler_path}")
                    print(f"   –ú–µ—Ç—Ä–∏–∫–∏: accuracy={metrics.get('accuracy', metrics.get('val_accuracy', 0)):.4f}")
                    if 'feature_names' in metrics:
                        print(f"   –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ —Ñ–∏—á–µ–π: {len(metrics['feature_names'])}")
                        if 'xgb' in model_type and len(metrics['feature_names']) > 100:
                            print(
                                f"   ‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: XGBoost –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–∏–ª–∞ {len(metrics['feature_names'])} —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö —Ñ–∏—á–µ–π")
                            print(f"   ‚ö†Ô∏è  –ü—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–∏ –∂–µ —Ñ–∏—á–∏!")
                return True
            else:
                if verbose:
                    print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏ –≤ –±–∞–∑—É")
                return False

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
            import traceback
            traceback.print_exc()
            return False

    def load_model(self, model_id: str, verbose: bool = True) -> Tuple[Any, Any]:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Å–∫–µ–π–ª–µ—Ä–∞
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–µ—à–∞
            if model_id in self.model_cache:
                if verbose:
                    self.log(f"Loading model {model_id} from cache")
                return self.model_cache[model_id]

            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏ –∏–∑ –ë–î
            models_df = self.db.get_available_models(active_only=False, verbose=verbose)

            if models_df.empty:
                self.log(f"Model {model_id} not found in database", 'error')
                return None, None

            model_info = models_df[models_df['model_id'] == model_id]
            if model_info.empty:
                self.log(f"Model {model_id} not found in database", 'error')
                return None, None

            model_info = model_info.iloc[0]
            model_path = model_info['model_path']
            model_type = model_info['model_type']

            # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–∫–µ–π–ª–µ—Ä–∞
            scaler_path = model_path.replace('.h5', '_scaler.pkl').replace('.pkl', '_scaler.pkl')

            scaler = None
            if os.path.exists(scaler_path):
                with open(scaler_path, 'rb') as f:
                    scaler = pickle.load(f)

            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
            model = None
            if 'lstm' in model_type:
                if os.path.exists(model_path):
                    try:
                        model = load_model(model_path)
                    except:
                        # –ü–æ–ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞–∫ .keras
                        keras_path = model_path.replace('.h5', '.keras')
                        if os.path.exists(keras_path):
                            model = load_model(keras_path)
            elif 'xgb' in model_type:
                if os.path.exists(model_path):
                    with open(model_path, 'rb') as f:
                        model = pickle.load(f)

            if model is None:
                self.log(f"Failed to load model from {model_path}", 'error')
                return None, None

            # –í–ê–ñ–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–∏—á–∏ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            if 'metrics' in model_info and model_info['metrics']:
                try:
                    metrics = json.loads(model_info['metrics'])

                    # –î–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
                    if 'feature_names' in metrics:
                        model.feature_names = metrics['feature_names']
                        if verbose:
                            print(f"  üíæ –ó–∞–≥—Ä—É–∂–µ–Ω—ã —Ñ–∏—á–∏ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {len(model.feature_names)} —Ñ–∏—á–µ–π")

                    # –î–ª—è XGBoost –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                    if 'xgb' in model_type:
                        if 'base_feature_names' in metrics:
                            model.base_feature_names = metrics['base_feature_names']
                            if verbose:
                                print(f"  üíæ –ë–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏: {len(model.base_feature_names)} —Ñ–∏—á–µ–π")

                        # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ–º —Ñ–∏—á–∏ –¥–ª—è XGBoost
                        if hasattr(model, 'feature_names'):
                            current_feature_count = len(model.feature_names)

                            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–∂–∏–¥–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏—á–µ–π
                            lookback_window = config.model.LOOKBACK_WINDOW
                            if hasattr(model, 'base_feature_names'):
                                base_count = len(model.base_feature_names)
                                expected_expanded = base_count * lookback_window
                            else:
                                # –ü—ã—Ç–∞–µ–º—Å—è –≤—ã—á–∏—Å–ª–∏—Ç—å
                                base_count = len([f for f in model.feature_names if '_t-' not in str(f)])
                                expected_expanded = base_count * lookback_window

                            if verbose:
                                print(f"  üîç –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Ñ–∏—á–µ–π XGBoost:")
                                print(f"     –¢–µ–∫—É—â–∏–µ —Ñ–∏—á–∏: {current_feature_count}")
                                print(f"     –û–∂–∏–¥–∞–µ—Ç—Å—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö: {expected_expanded}")
                                print(f"     Lookback window: {lookback_window}")

                            # –ï—Å–ª–∏ —Ñ–∏—á–µ–π –º–∞–ª–æ (–±–∞–∑–æ–≤—ã–µ), —Å–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ
                            if current_feature_count < 100 and current_feature_count * lookback_window == expected_expanded:
                                if verbose:
                                    print(f"  üîß –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –±–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏, —Å–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ...")

                                # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏
                                base_features = []
                                if hasattr(model, 'base_feature_names'):
                                    base_features = model.base_feature_names
                                elif hasattr(model, 'feature_names'):
                                    base_features = model.feature_names

                                if base_features:
                                    # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏
                                    expanded_features = []
                                    for i in range(lookback_window):
                                        for feature in base_features:
                                            expanded_features.append(f"{feature}_t-{lookback_window - i - 1}")

                                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–∞ –Ω–∞–±–æ—Ä–∞
                                    model.expanded_feature_names = expanded_features
                                    model.feature_names = expanded_features  # –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∏—á–∏ = —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ
                                    model._features = base_features  # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏

                                    if verbose:
                                        print(f"  ‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(expanded_features)} —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö —Ñ–∏—á–µ–π")
                                        print(
                                            f"  üìã –§–æ—Ä–º—É–ª–∞: {len(base_features)} √ó {lookback_window} = {len(expanded_features)}")
                            else:
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ª–∏ —ç—Ç–æ —É–∂–µ —Ñ–∏—á–∏
                                if current_feature_count > 100:
                                    if verbose:
                                        print(f"  ‚úÖ –ü–æ—Ö–æ–∂–µ –Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏ ({current_feature_count} —Ñ–∏—á–µ–π)")
                                    model.expanded_feature_names = model.feature_names
                                else:
                                    if verbose:
                                        print(f"  ‚ö†Ô∏è  –ù–µ–ø–æ–Ω—è—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∏—á–µ–π: {current_feature_count} —Ñ–∏—á–µ–π")
                except Exception as e:
                    if verbose:
                        print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Ñ–∏—á–µ–π: {e}")

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–ª—è XGBoost
            if 'xgb' in model_type and verbose:
                print(f"  üîß –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ XGBoost –º–æ–¥–µ–ª–∏:")
                if hasattr(model, 'feature_names'):
                    print(f"     feature_names: {len(model.feature_names)} —Ñ–∏—á–µ–π")
                if hasattr(model, 'expanded_feature_names'):
                    print(f"     expanded_feature_names: {len(model.expanded_feature_names)} —Ñ–∏—á–µ–π")
                if hasattr(model, 'base_feature_names'):
                    print(f"     base_feature_names: {len(model.base_feature_names)} —Ñ–∏—á–µ–π")

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏—á–µ–π –æ–∂–∏–¥–∞–Ω–∏—è–º –º–æ–¥–µ–ª–∏
                if hasattr(model, 'feature_names') and hasattr(scaler, 'n_features_in_'):
                    model_features = len(model.feature_names)
                    scaler_features = scaler.n_features_in_
                    if model_features != scaler_features:
                        print(f"  ‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ú–æ–¥–µ–ª—å –∏ —Å–∫–µ–π–ª–µ—Ä –∏–º–µ—é—Ç —Ä–∞–∑–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏—á–µ–π!")
                        print(f"     –ú–æ–¥–µ–ª—å: {model_features} —Ñ–∏—á–µ–π")
                        print(f"     –°–∫–µ–π–ª–µ—Ä: {scaler_features} —Ñ–∏—á–µ–π")

            # –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ
            self.model_cache[model_id] = (model, scaler)

            if verbose:
                print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_id} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                print(f"   –¢–∏–ø: {model_type}")
                print(f"   –ü—É—Ç—å: {model_path}")
                if hasattr(model, 'feature_names'):
                    print(f"   –§–∏—á–µ–π –≤ –º–æ–¥–µ–ª–∏: {len(model.feature_names)}")
                    if 'xgb' in model_type and len(model.feature_names) < 100:
                        print(f"   ‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: XGBoost –º–æ–¥–µ–ª—å –∏–º–µ–µ—Ç —Ç–æ–ª—å–∫–æ {len(model.feature_names)} —Ñ–∏—á–µ–π")
                        print(
                            f"   ‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –æ–∂–∏–¥–∞–µ—Ç {len(model.feature_names) * config.model.LOOKBACK_WINDOW} —Ñ–∏—á–µ–π –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏!")

            return model, scaler

        except Exception as e:
            self.log(f"Error loading model: {e}", 'error')
            import traceback
            traceback.print_exc()
            return None, None

    def prepare_sequences_with_features(self, df: pd.DataFrame, target_column: str,
                                        lookback_window: int = 60,
                                        use_advanced_features: bool = True,
                                        feature_columns: List[str] = None,  # –ù–æ–≤—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
                                        verbose: bool = True) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –≤–æ–∑–≤—Ä–∞—Ç–æ–º –∏–º–µ–Ω —Ñ–∏—á–µ–π
        """
        try:
            # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã —Ñ–∏—á–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
            if feature_columns is not None:
                feature_columns_to_use = feature_columns
            else:
                # –°—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞ (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
                base_features = ['close', 'volume', 'returns']
                tech_indicators = [col for col in df.columns
                                   if any(indicator in col for indicator in
                                          ['SMA', 'EMA', 'RSI', 'MACD', 'BB', 'ATR', 'OBV', 'ADX'])]
                advanced_features = []
                if use_advanced_features:
                    advanced_features = [col for col in df.columns
                                         if col.startswith('FEATURE_') or
                                         any(x in col for x in
                                             ['volatility', 'spread', 'skew', 'kurtosis', 'volume_profile'])]

                feature_columns_to_use = base_features + tech_indicators + advanced_features

                # –ò–°–ö–õ–Æ–ß–ê–ï–ú –í–†–ï–ú–ï–ù–ù–´–ï –§–ò–ß–ò –î–õ–Ø –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò
                temporal_features = ['HOUR', 'DAY_OF_WEEK', 'MONTH', 'HOUR_OF_DAY', 'DAY', 'WEEK', '_SIN', '_COS']
                feature_columns_to_use = [col for col in feature_columns_to_use
                                          if not any(temp in col for temp in temporal_features)]

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∏—á–µ–π –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            self.last_feature_columns = feature_columns_to_use.copy()

            # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
            feature_columns_to_use = [col for col in feature_columns_to_use if col in df.columns]

            missing_features = [col for col in self.last_feature_columns if col not in df.columns]
            if missing_features and verbose:
                print(f"   ‚ö†Ô∏è  –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∏—á–∏: {missing_features[:5]}...")

            if len(feature_columns_to_use) == 0:
                print("   ‚ùå –ù–µ—Ç —Ñ–∏—á–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                return np.array([]), np.array([]), []

            print(f"   üîç –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {len(feature_columns_to_use)} —Ñ–∏—á–µ–π")
            if verbose:
                print(f"   üìã –§–∏—á–∏: {', '.join(feature_columns_to_use[:10])}" +
                      ("..." if len(feature_columns_to_use) > 10 else ""))

            # –°–æ–∑–¥–∞–µ–º –º–∞—Å—Å–∏–≤—ã
            X = []
            y = []

            data_features = df[feature_columns_to_use].values
            data_target = df[target_column].values

            for i in range(lookback_window, len(df)):
                X.append(data_features[i - lookback_window:i])
                y.append(data_target[i])

            if len(X) == 0:
                print("   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
                return np.array([]), np.array([]), []

            X_array = np.array(X)
            y_array = np.array(y)

            print(f"   üìê –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å X: {X_array.shape}")
            print(f"   üìê –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å y: {y_array.shape}")

            return X_array, y_array, feature_columns_to_use

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {e}")
            import traceback
            traceback.print_exc()
            return np.array([]), np.array([]), []